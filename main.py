# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RCNZUPrVyi-wV0BW0KX9JtD1woumNhAd
"""

import numpy as np
from math import sqrt

# Класс треугольного числа
class TriFuzzy:
    def __init__(self,a,b,c):
        self.a,self.b,self.c = a,b,c
    def left(self,k):
        return self.b - (self.b - self.a) * (1 - k)
    def right(self,k):
        return self.b + (self.c - self.b) * (1 - k)
    def __add__(self,other):
        return TriFuzzy(self.a + other.a, self.b + other.b, self.c + other.c)
    def scale(self, w):
        return TriFuzzy(self.a * w, self.b * w, self.c * w)
    def __repr__(self):
        return f"TriFuzzy({self.a:.4f}, {self.b:.4f}, {self.c:.4f})"

from math import sqrt
from scipy.stats import t

def expert_criteria_vectors(Alternative):
    CriteriaExpertValues = {}
    ExpertsSet = set()

    def collect_leaf_criteria(Node, Path=""):
        if not isinstance(Node, dict):
            return

        IsLeaf = all(
            isinstance(Value, (int, float))
            for Value in Node.values()
        )

        if IsLeaf:
            CriteriaName = Path
            CriteriaExpertValues[CriteriaName] = {}

            for ExpertId, Value in Node.items():
                CriteriaExpertValues[CriteriaName][ExpertId] = Value
                ExpertsSet.add(ExpertId)
            return

        for Key, SubNode in Node.items():
            NewPath = f"{Path}.{Key}" if Path else Key
            collect_leaf_criteria(SubNode, NewPath)

    collect_leaf_criteria(Alternative['t'])

    SortedCriteria = sorted(CriteriaExpertValues.keys())

    ExpertVectors = {}
    for ExpertId in sorted(ExpertsSet):
        ExpertVectors[ExpertId] = [
            CriteriaExpertValues[Criteria][ExpertId]
            for Criteria in SortedCriteria
        ]

    return ExpertVectors


def expert_statistics(ExpertVectors):
    ExpertStats = {}

    for ExpertId, Values in ExpertVectors.items():
        N = len(Values)
        MeanValue = sum(Values) / N
        Variance = sum((V - MeanValue) ** 2 for V in Values) / (N - 1)
        StdValue = sqrt(Variance)
        ExpertStats[ExpertId] = (MeanValue, StdValue, N)

    return ExpertStats

# Вычисляем доверительные интервалы экспертов
def expert_confidence_intervals(ExpertStats, Alpha=0.05):
    ConfidenceIntervals = {}

    for ExpertId, (MeanValue, StdValue, N) in ExpertStats.items():
        DegreesOfFreedom = N - 1
        TCritical = t.ppf(1 - Alpha / 2, DegreesOfFreedom)

        Margin = TCritical * StdValue / sqrt(N)
        LowerBound = MeanValue - Margin
        UpperBound = MeanValue + Margin

        ConfidenceIntervals[ExpertId] = (LowerBound, UpperBound)

    return ConfidenceIntervals

# Проверка перечисления интервалов А и В
def intervals_overlap(IntervalA, IntervalB):
    A_min, A_max = IntervalA
    B_min, B_max = IntervalB

    return not (A_max < B_min or B_max < A_min)

# Кол-во пересечений
def count_ci_overlaps(ConfidenceIntervals):
    # print("ConfidenceIntervals = ", ConfidenceIntervals)
    OverlapCounts = {}
    Experts = list(ConfidenceIntervals.keys())

    for ExpertA in Experts:
        Count = 0
        IntervalA = ConfidenceIntervals[ExpertA]

        for ExpertB in Experts:
            if ExpertA == ExpertB:
                continue

            IntervalB = ConfidenceIntervals[ExpertB]

            if intervals_overlap(IntervalA, IntervalB):
                Count += 1

        OverlapCounts[ExpertA] = Count

    return OverlapCounts


def detect_biased_experts_ci_overlap(
    Alternative,
    BThreshold=1,   # минимальное число пересечений
    Alpha=0.05      # уровень значимости (по умолчанию, 0.05)
):
    ExpertVectors = expert_criteria_vectors(Alternative)
    ExpertStats = expert_statistics(ExpertVectors)
    ConfidenceIntervals = expert_confidence_intervals(
        ExpertStats,
        Alpha
    )
    # print("ExpertStats = ", ExpertStats)
    # print("ConfidenceIntervals = ", ConfidenceIntervals)
    OverlapCounts = count_ci_overlaps(ConfidenceIntervals)

    BiasedExperts = [
        ExpertId
        for ExpertId, Count in OverlapCounts.items()
        if Count < BThreshold
    ]

    return {
        'biased_experts': BiasedExperts,
        'confidence_intervals': ConfidenceIntervals,
        'overlap_counts': OverlapCounts
    }

import copy

def exclude_expert_from_alternative(Alternative, ExpertId, ChangeParam='delete'):
    NewAlternative = copy.deepcopy(Alternative)

    def remove_expert_from_node(Node):
        if not isinstance(Node, dict):
            return

        # Проверка: листовой критерий (значения — оценки экспертов)
        IsLeaf = all(
            isinstance(Value, (int, float))
            for Value in Node.values()
        )

        if IsLeaf:
            if ExpertId in Node:
                if ChangeParam == 'delete':
                  del Node[ExpertId]
                else:
                  Node[ExpertId] = 0.5 * (ChangeParam + Node[ExpertId])
            return

        # Иначе — рекурсивно обходим дальше
        for SubNode in Node.values():
            remove_expert_from_node(SubNode)

    remove_expert_from_node(NewAlternative['t'])

    return NewAlternative

def abs_criteria(Alternative):
    LeafCriteriaValues = {}

    def collect_leaf_criteria(Node, Path=""):
        if not isinstance(Node, dict):
            return
        # Проверяем: является ли Node листом (значения — оценки экспертов)
        IsLeaf = all(
            isinstance(Value, (int, float))
            for Value in Node.values()
        )
        if IsLeaf:
            CriteriaName = Path
            ExpertValues = list(Node.values())
            MeanValue = sum(ExpertValues) / len(ExpertValues)
            LeafCriteriaValues[CriteriaName] = MeanValue
            return
        # Иначе — углубляемся
        for Key, SubNode in Node.items():
            NewPath = f"{Path}.{Key}" if Path else Key
            collect_leaf_criteria(SubNode, NewPath)

    # Запускаем обход с корня матрицы оценок
    collect_leaf_criteria(Alternative['t'])

    # Сортируем критерии для детерминированного вектора
    SortedCriteria = sorted(LeafCriteriaValues.keys())

    ResultVector = [
        LeafCriteriaValues[Criteria]
        for Criteria in SortedCriteria
    ]

    return ResultVector

def trinum_to_crisp(FuzzyNum, a, b, c):
    return (a*FuzzyNum.a + b*FuzzyNum.b + c*FuzzyNum.c) / (a + b + c)

def defuzzify_centroid(ExpertVectors, DefuzzyRatio=[4, 1, 1]):
    NewExpertVectors = {}
    for Key in list(ExpertVectors.keys()):
        FuzzyVector = []
        for item in ExpertVectors[Key]:
            FuzzyVector.append(TriFuzzy(item-0.1, item, item+0.1))

        Result = []
        for f in FuzzyVector:
           Value = trinum_to_crisp(f, DefuzzyRatio[0], DefuzzyRatio[1], DefuzzyRatio[2])
           Result.append(Value)
        NewExpertVectors[Key] = Result

    return NewExpertVectors

def expert_centroid_confidence_intervals(ExpertVectors, delta=0.05):
    ConfidenceIntervals = {}
    for Key in list(ExpertVectors.keys()):
        MinValue = min(ExpertVectors[Key])
        MaxValue = max(ExpertVectors[Key])

        ConfidenceIntervals[Key] = (MinValue, MaxValue)
    print(ConfidenceIntervals)
    return ConfidenceIntervals

def detect_biased_experts_ci_overlap_with_centroid(
    Alternative,
    BThreshold=1,   # минимальное число пересечений
    Delta=0.05      # Отклонение оценки эксперта (по умолчанию, 0.05)
):
    ExpertVectors = expert_criteria_vectors(Alternative)
    DefyzziExpertVectors = defuzzify_centroid(ExpertVectors, [1, 2, 10])
    ExpertStats = expert_statistics(DefyzziExpertVectors)
    ConfidenceIntervals = expert_confidence_intervals(
        ExpertStats,
        Delta
    )
    # ConfidenceIntervals = expert_centroid_confidence_intervals(DefyzziExpertVectors)
    OverlapCounts = count_ci_overlaps(ConfidenceIntervals)

    BiasedExperts = [
        ExpertId
        for ExpertId, Count in OverlapCounts.items()
        if Count < BThreshold
    ]

    return {
        'biased_experts': BiasedExperts,
        'confidence_intervals': ConfidenceIntervals,
        'overlap_counts': OverlapCounts
    }

def expert_maxmin_confidence_intervals(ExpertVectors, delta=0.05):
    ConfidenceIntervals = {}
    for Key in list(ExpertVectors.keys()):
        MinValue = min(ExpertVectors[Key]) - delta
        MaxValue = max(ExpertVectors[Key]) + delta

        ConfidenceIntervals[Key] = (MinValue, MaxValue)

    return ConfidenceIntervals

def expert_maxmin_vectors_with_maxmin(ExpertVectors, delta=0.05):
    ConfidenceIntervals = {}
    for Key in list(ExpertVectors.keys()):
        Vector = []
        for elem in ExpertVectors[Key]:
            Vector.append(elem - delta)
            Vector.append(elem + delta)
        # MinValue = min(ExpertVectors[Key]) - delta
        # MaxValue = max(ExpertVectors[Key]) + delta

        ConfidenceIntervals[Key] = Vector

    return ConfidenceIntervals

def detect_biased_experts_ci_overlap_with_maxmin(
    Alternative,
    BThreshold=1,   # минимальное число пересечений
    Delta=0.05      # Отклонение оценки эксперта (по умолчанию, 0.05)
):
    ExpertVectors = expert_criteria_vectors(Alternative)
    DefyzziExpertVectors = expert_maxmin_vectors_with_maxmin(ExpertVectors)
    ExpertStats = expert_statistics(DefyzziExpertVectors)
    # ConfidenceIntervals = expert_maxmin_confidence_intervals(ExpertVectors, Delta)
    ConfidenceIntervals = expert_confidence_intervals(ExpertStats, Delta)
    # print(ConfidenceIntervals)
    OverlapCounts = count_ci_overlaps(ConfidenceIntervals)

    BiasedExperts = [
        ExpertId
        for ExpertId, Count in OverlapCounts.items()
        if Count < BThreshold
    ]

    return {
        'biased_experts': BiasedExperts,
        'confidence_intervals': ConfidenceIntervals,
        'overlap_counts': OverlapCounts
    }

def expert_maxmin_with_expweigth_confidence_intervals(ExpertVectors, ExpertWeight, delta):
    ConfidenceIntervals = {}
    for Key in list(ExpertVectors.keys()):
        MinValue = min(ExpertVectors[Key]) - (2 * delta * (1 - ExpertWeight[Key]))
        MaxValue = max(ExpertVectors[Key]) + (2 * delta * (1 - ExpertWeight[Key]))

        ConfidenceIntervals[Key] = (MinValue, MaxValue)

    return ConfidenceIntervals

def expert_maxmin_vectors_with_expweigth(ExpertVectors, ExpertWeight, delta):
    ConfidenceIntervals = {}
    for Key in list(ExpertVectors.keys()):
        Vector = []
        for elem in ExpertVectors[Key]:
            Vector.append(elem - (2 * delta * (1 - ExpertWeight[Key])))
            Vector.append(elem + (2 * delta * (1 - ExpertWeight[Key])))
        # MinValue = min(ExpertVectors[Key]) - (2 * delta * (1 - ExpertWeight[Key]))
        # MaxValue = max(ExpertVectors[Key]) + (2 * delta * (1 - ExpertWeight[Key]))

        ConfidenceIntervals[Key] = Vector

    return ConfidenceIntervals

def detect_biased_experts_ci_overlap_with_expweigth(
    Alternative,
    BThreshold=1,   # минимальное число пересечений
    Delta=0.05      # Отклонение оценки эксперта (по умолчанию, 0.05)
):
    ExpertVectors = expert_criteria_vectors(Alternative)
    DefyzziExpertVectors = expert_maxmin_vectors_with_expweigth(
        ExpertVectors,
        Alternative['e'],
        Delta
    )
    ExpertStats = expert_statistics(DefyzziExpertVectors)
    # ConfidenceIntervals = expert_maxmin_with_expweigth_confidence_intervals(
    #     ExpertVectors,
    #     Alternative['e'],
    #     Delta
    # )
    ConfidenceIntervals = expert_confidence_intervals(ExpertStats, Delta)
    OverlapCounts = count_ci_overlaps(ConfidenceIntervals)

    BiasedExperts = [
        ExpertId
        for ExpertId, Count in OverlapCounts.items()
        if Count < BThreshold
    ]

    return {
        'biased_experts': BiasedExperts,
        'confidence_intervals': ConfidenceIntervals,
        'overlap_counts': OverlapCounts
    }

import random

def generate_alternative(
    name,
    biased_expert="e4",
    bias_type="high",        # "high" | "low"
    bias_strength=(0.05, 0.3),
    honest_noise=0.2
):
    Experts = ["e1", "e2", "e3", "e4"]
    HonestExperts = [e for e in Experts if e != biased_expert]
    Criteria = ["c11", "c12", "c2", "c3"]

    t = {
        "c1": {"c11": {}, "c12": {}},
        "c2": {},
        "c3": {}
    }

    for c in Criteria:
        base = random.uniform(0.3, 0.7)

        honest_values = {}
        for e in HonestExperts:
            val = base + random.uniform(-honest_noise, honest_noise)
            val = max(0.0, min(1.0, val))
            honest_values[e] = val

        if bias_type == "high":
            reference = max(honest_values.values())
            biased_val = reference + random.uniform(*bias_strength)
        else:
            reference = min(honest_values.values())
            biased_val = reference - random.uniform(*bias_strength)

        biased_val = max(0.0, min(1.0, biased_val))

        # Заполняем структуру
        for e, v in honest_values.items():
            if c in ["c11", "c12"]:
                t["c1"][c][e] = v
            else:
                t[c][e] = v

        if c in ["c11", "c12"]:
            t["c1"][c][biased_expert] = biased_val
        else:
            t[c][biased_expert] = biased_val

    alternative = {
        "name": name,
        "e": {
            "c1": 0.6, "c2": 0.4, "c3": 0.5,
            "c11": 0.7, "c12": 0.3,
            "e1": 0.5, "e2": 0.3, "e3": 0.2, "e4": 0.9
        },
        "t": t,
        "biased_expert": biased_expert
    }

    return alternative

# For create new dataset with N alternatives
def generate_dataset(N=40):
    Dataset = []

    for i in range(N):
        biased_expert = random.choice(["e1", "e2", "e3", "e4"])
        bias_type = random.choice(["high", "low"])

        alt = generate_alternative(
            name=f"A{i+1}",
            biased_expert=biased_expert,
            bias_type=bias_type
        )
        Dataset.append(alt)

    return Dataset

def evaluate_method_f1(
    Dataset,
    DetectionFunction,
    **kwargs
):
    TP = FP = FN = 0

    for alt in Dataset:
        true_biased = alt["biased_expert"]
        result = DetectionFunction(alt, **kwargs)
        detected = result["biased_experts"]

        if true_biased in detected:
            TP += 1
        else:
            FN += 1

        for e in detected:
            if e != true_biased:
                FP += 1

    if TP == 0:
        return 0.0

    return 2 * TP / (2 * TP + FP + FN)

with open("./50_alternatives.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

print(f"Upload {len(dataset)} alternatives")

f1_centroid = evaluate_method_f1(
    dataset,
    detect_biased_experts_ci_overlap_with_centroid,
    Delta=0.05
)

f1_maxmin = evaluate_method_f1(
    dataset,
    detect_biased_experts_ci_overlap_with_maxmin,
    Delta=0.05
)

f1_weighted = evaluate_method_f1(
    dataset,
    detect_biased_experts_ci_overlap_with_expweigth,
    Delta=0.05
)

print("Dataset alt:", dataset[0])
print("Centroid F1:", f1_centroid)
print("MaxMin F1:", f1_maxmin)
print("Weighted MaxMin F1:", f1_weighted)



